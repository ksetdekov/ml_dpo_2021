{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "BASE_ENV",
      "language": "python",
      "name": "base_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "hw02.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksetdekov/ml_dpo_2021/blob/master/hw/hw2/hw02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SHu26g4UI68"
      },
      "source": [
        "# Домашнее задание №2\n",
        "\n",
        "Задание выполнил: ```(Кирилл Сетдеков)```\n",
        "\n",
        "### Общая информация\n",
        "\n",
        "__Дата выдачи:__ 25.04.2021\n",
        "\n",
        "__Дедлайн:__ 05.05.2021 23:59\n",
        "\n",
        "# О задании\n",
        "\n",
        "В этом домашнем задании вы реализуете градиентный спуск и стохастический градиентный спуск для обучения линейной регрессии, попрактикуетесь в анализе данных и их обработке, а также примените модели линейной регрессии на практике и проанализируете результаты.\n",
        "\n",
        "Обратите внимание, что вам не только нужно написать код, но и в некоторых местах ответить на вопросы.\n",
        "\n",
        "\n",
        "### Оценивание и штрафы\n",
        "\n",
        "\n",
        "#### Gradient Descent [8 баллов]\n",
        "* [Задание 1](#task1) [1 балл]\n",
        "* [Задание 2](#task2) [2 балла]\n",
        "* [Задание 3](#task3) [3 балла]\n",
        "* [Задание 4](#task4) [2 балла]\n",
        "\n",
        "#### Linear Regression [8 баллов] + 2 бонусных\n",
        "* [Задание 1](#task21) [1 балл]\n",
        "* [Задание 2](#task22) [1 балл]\n",
        "* [Задание 3](#task23) [1 балл]\n",
        "* [Задание 4](#task24) [1 балл]\n",
        "* [Задание 5](#task25) [1 балл]\n",
        "* [Задание 6](#task26) [1 балл]\n",
        "* [Задание 7](#task27) [1 балл]\n",
        "* [Задание 8](#task28) [1 балл]\n",
        "* [Задание 9](#task29) [2 баллa]\n",
        "\n",
        "\n",
        "Итоговая оценка за домашнюю работу вычисляется по формуле: $$s \\cdot \\frac{10}{16},$$ где $s$ - сумма набранных балов. С учетом бонусов, за домашнее задание можно получить > 10 балов. \n",
        "\n",
        "За сдачу задания позже срока на итоговую оценку за задание накладывается штраф в размере 0.25 балла в день (от оценки в 10 бальной шкале), но получить отрицательную оценку нельзя.\n",
        "\n",
        "__Внимание!__ Домашнее задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов.\n",
        "\n",
        "### Формат сдачи\n",
        "\n",
        "Загрузка файлов с решениями происходит в системе [Anytask](https://anytask.org/course/811). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JRnTcvqUI7I"
      },
      "source": [
        "## Часть 1. Gradient Descent\n",
        "\n",
        "Среднеквадратичная ошибка на всем датасете $(X, y) = \\{(x_1, y_1), \\ldots, (x_\\ell, y_\\ell)\\}$ для линейной регрессии с вектором весов $w$ (в предположении, что в данных есть единичный признак) представляется следующим образом:\n",
        "\n",
        "$$\n",
        "Q(w) = \\frac{1}{\\ell}\\|Xw - y\\|_2^2\n",
        "$$\n",
        "\n",
        "Градиент данного функционала потерь по $w$:\n",
        "\n",
        "$$\n",
        "\\nabla_w Q(w) = \\frac{2}{\\ell}X^T(Xw - y)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyAX4U9WUI7J"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "058cM0PqUI7K"
      },
      "source": [
        "Для тестирования реализованных вами методов будет использоваться датасет о ценах домов из `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgtGf0m_UI7L"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# загружаем данные\n",
        "data = load_boston()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# делим данные на обучающую и тестовую часть\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.array(X), y, test_size=0.3, random_state=10)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujjl1AFGUI7M"
      },
      "source": [
        "### Задание 1 <a id=\"task1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwQksf3QUI7N"
      },
      "source": [
        "С линейной регрессией удобно работать в матрично-векторном виде, если предположить, что в данных есть единичный признак. Реализуйте функцию, которая принимает на вход матрицу объекты-признаки, и добавляет в нее столбец (первый), заполненный единицами. Примените написанную функцию к обучающей и тестовой части имеющихся данных.\n",
        "\n",
        "Например, если на вход подается матрица \n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "2 & 4 & 4200\\\\\n",
        "0 & 10 & 5000\\\\\n",
        "2 & 2 & 1000\\\\\n",
        "\\end{pmatrix},\n",
        "$$\n",
        "то на выходе будет матрица\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "1 & 2 & 4 & 4200\\\\\n",
        "1 & 0 & 10 & 5000\\\\\n",
        "1 & 2 & 2 & 1000\\\\\n",
        "\\end{pmatrix}$$\n",
        "\n",
        "**Hint** \n",
        "- Вам могут пригодиться функции `np.concatenate`, `np.ones`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR77KVYjUI7N"
      },
      "source": [
        "# create function\n",
        "def add_identity_feature(X):\n",
        "    '''\n",
        "    INPUT:\n",
        "    X - np.array shape=(ℓ, d-1)\n",
        "    \n",
        "    OUTPUT:\n",
        "    X - np.array shape=(ℓ, d)\n",
        "    '''\n",
        "    leftmost = np.ones((X.shape[0], 1))\n",
        "    X = np.append(leftmost, X, axis=1)\n",
        "    return X\n",
        "\n",
        "X_train = add_identity_feature(X_train)\n",
        "X_test = add_identity_feature(X_test)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6fyC3lBUI7O"
      },
      "source": [
        "# test yourself\n",
        "assert X_train.shape == (354, 14)\n",
        "assert len(X_train) == X_train.sum(axis=0)[0]\n",
        "\n",
        "assert X_test.shape == (152, 14)\n",
        "assert len(X_test) == X_test.sum(axis=0)[0]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rerObh2tUI7O"
      },
      "source": [
        "### Задание 2 <a id=\"task2\"></a>\n",
        "\n",
        "Реализуйте подсчет среднеквадратичной функции потерь и ее градиента по весам. Формулы должны быть реализованы в явном виде с помощью `numpy`, без использования циклов `for`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PyJmqSLUI7P"
      },
      "source": [
        "**Hint** \n",
        "- Для умножения матрицы на вектор (или матрицы на матрицу) можно использовать функцию `np.dot`. \n",
        "- Не забывайте, что оператор `*` используется только для поэлементного умножения.\n",
        "- Также может пригодиться функция `np.linalg.norm`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj2nLG7xUI7P"
      },
      "source": [
        "def mse_loss(X, y, w):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "    X - np.array shape=(ℓ, d)\n",
        "    y - np.array shape=(ℓ,)\n",
        "    w - np.array shape=(d,)\n",
        "    \n",
        "    OUTPUT:\n",
        "    loss - scalar\n",
        "    \"\"\"\n",
        "    \n",
        "    y_hat = X @ w\n",
        "    loss = np.linalg.norm(y - y_hat)\n",
        "    loss = (loss**2) / X.shape[0]\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def mse_grad(X, y, w):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "    X - np.array shape=(ℓ, d)\n",
        "    y - np.array shape=(ℓ,)\n",
        "    w - np.array shape=(d,)\n",
        "    \n",
        "    OUTPUT:\n",
        "    grad - np.array shape=(d,)\n",
        "    \"\"\"\n",
        "\n",
        "    grad = (2/ X.shape[0]) *( X.T @ X @ w - X.T @ y)\n",
        "    return grad"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foMHZhhgUI7Q"
      },
      "source": [
        "Посчитайте `result_mse_sklearn` используя функцию `mean_squared_error` из sklearn, чтобы сравнить c результатом вашей функции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiUrWE8qUI7R"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "w = np.zeros(X_train.shape[1])\n",
        "\n",
        "result_mse_custom = mse_loss(X_train, y_train, w)\n",
        "result_mse_grad = mse_grad(X_train, y_train, w)\n",
        "\n",
        "\n",
        "result_mse_sklearn = mean_squared_error(y_train, np.dot(X_train,w))\n",
        "\n",
        "assert (X_train @ w).shape == y_train.shape\n",
        "assert np.allclose(result_mse_custom, result_mse_sklearn)\n",
        "assert np.allclose(result_mse_grad, np.array([-43.9424,  -99.9887, -675.8766, -444.0919,   -3.0395,  \n",
        "                                              -23.5569,   -282.6831,  -2829.3631,   -177.206 ,  -373.6983,\n",
        "                                              -16820.8966,   -796.6377, -16136.5175,   -477.6308]),rtol=1e-03)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUcl44nwUI7S"
      },
      "source": [
        "### Задание 3  <a id=\"task3\"></a>\n",
        "\n",
        "Реализуйте класс для линейной регрессии. Он должен поддерживать обучение с помощью градиентного спуска (метод `fit`) и предсказание для выборки (метод `predict`).\n",
        "\n",
        "Метод `fit` должен возвращать список из значений функционала потерь на каждой итерации градиентного спуска. Напомним, что алгоритм градиентного спуска состоит из трех этапов:\n",
        "\n",
        "1. **Инициализация.** В данном случае вектор весов вы можете инициализировать нулями.\n",
        "2. **Шаг градиентного спуска.** Обновление весов с помощью антиградиента функционала потерь с заданной длиной шага (параметр `eta`). \n",
        "$$\n",
        "w^{t+1} = w^t - \\eta \\nabla Q(w^t)\n",
        "$$\n",
        "Для того, чтобы нарисовать график зависимости значения функционала потерь от номера итерации, необходимо на каждом шаге запоминать значение функционала потерь (обновлять список `self.loss_history`). На этом этапе вы можете использовать уже реализованные вами функции `mse_loss` и `mse_grad` из предыдущего задания.\n",
        "\n",
        "3. **Остановка.** В качестве критерия останова используйте $\\|w^t - w^{t - 1}\\| < \\varepsilon$ (значение $\\varepsilon$ задается параметром `tol`). В качестве дополнительного критерия останова используйте максимальное число итераций (параметр `max_iter`). Таким образом, даже если первый критерий не сработает, алгоритм все равно остановится после `max_iter` шагов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep78MfCDUI7U"
      },
      "source": [
        "class CustomLinearRegressionGD:\n",
        "    def __init__(self):\n",
        "        self.w = None\n",
        "        self.loss_history = None\n",
        "        \n",
        "    def fit(self, X, y, max_iter=100, tol=1e-6, eta=1e-6):\n",
        "        \"\"\"\n",
        "        ARGUMENTS:\n",
        "        max_iter - максимальное число шагов градиентного спуска\n",
        "        tol - значение эпсилон для критерия останова\n",
        "        eta - длина шага градиентного спуска (learning rate)\n",
        "        X - np.array of shape (ℓ, d)\n",
        "        y - np.array of shape (ℓ,)\n",
        "        \n",
        "        OUTPUT:\n",
        "        loss_hist - list \n",
        "        \"\"\"\n",
        "        \n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        self.loss_history = [mse_loss(X, y, self.w)]\n",
        "        for t in range(max_iter):\n",
        "            next_w = self.w - eta * mse_grad(X, y, self.w)\n",
        "            self.w = next_w \n",
        "            self.loss_history.append(mse_loss(X, y, self.w))\n",
        "        \n",
        "        return self.loss_history\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        ARGUMENTS:\n",
        "        X_test - np.array of shape (ℓ, d)\n",
        "        \n",
        "        OUTPUT:\n",
        "        y_pred - np.array of shape (ℓ,)\n",
        "        \"\"\"\n",
        "        # your code here\n",
        "        return y_pred"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfIlvqz4UI7U"
      },
      "source": [
        "Проверьте работу вашего класса. \n",
        "- Обучите линейную регрессию на обучающей части выборки.\n",
        "- Постройте график зависимости значения функционала потерь от итерации градиентного спуска. \n",
        "- Подберите вручную оптимальную длину шага градиентного спуска и посчитайте MSE на тестовой части с помощью функции `mean_squared_error` из `sklearn`. \n",
        "- Добейтесь того, чтобы значение MSE не превышало 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFgi9JJiUI7V",
        "outputId": "a2c068d2-6005-4ee5-8e00-367dca0c8fd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lr = CustomLinearRegressionGD()\n",
        "lr.fit(X, y)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[592.1469169960475,\n",
              " 196.24724722871895,\n",
              " 136.88337490947555,\n",
              " 124.94909597282222,\n",
              " 119.97944663875393,\n",
              " 116.28500716597888,\n",
              " 113.04250375902105,\n",
              " 110.11172393529547,\n",
              " 107.45028015685948,\n",
              " 105.03155379673996,\n",
              " 102.83302819063091,\n",
              " 100.8344831484161,\n",
              " 99.01758452882862,\n",
              " 97.36568339068513,\n",
              " 95.86365760331825,\n",
              " 94.49777133940988,\n",
              " 93.25554791822812,\n",
              " 92.12565437090629,\n",
              " 91.09779659664525,\n",
              " 90.1626241310744,\n",
              " 89.3116436445309,\n",
              " 88.53714036984591,\n",
              " 87.83210673272926,\n",
              " 87.19017752449857,\n",
              " 86.60557101742577,\n",
              " 86.07303547794999,\n",
              " 85.587800582943,\n",
              " 85.1455332895715,\n",
              " 84.74229775050175,\n",
              " 84.37451890361659,\n",
              " 84.03894939940746,\n",
              " 83.7326395600823,\n",
              " 83.45291009247553,\n",
              " 83.19732730232502,\n",
              " 82.96368058061759,\n",
              " 82.7499619537273,\n",
              " 82.55434750816141,\n",
              " 82.37518051807156,\n",
              " 82.21095611943996,\n",
              " 82.06030738915985,\n",
              " 81.92199270022508,\n",
              " 81.79488423605086,\n",
              " 81.67795755766811,\n",
              " 81.57028212727816,\n",
              " 81.47101270049797,\n",
              " 81.37938150766557,\n",
              " 81.29469115187284,\n",
              " 81.21630815802527,\n",
              " 81.14365711324896,\n",
              " 81.07621534443783,\n",
              " 81.01350808370192,\n",
              " 80.95510407699147,\n",
              " 80.90061159527201,\n",
              " 80.84967481134814,\n",
              " 80.8019705088194,\n",
              " 80.75720509272001,\n",
              " 80.7151118741894,\n",
              " 80.67544860405218,\n",
              " 80.63799523249187,\n",
              " 80.60255187409072,\n",
              " 80.56893695941208,\n",
              " 80.5369855560241,\n",
              " 80.50654784343224,\n",
              " 80.47748772781279,\n",
              " 80.44968158373123,\n",
              " 80.42301711120552,\n",
              " 80.39739229754048,\n",
              " 80.37271447432906,\n",
              " 80.34889946089724,\n",
              " 80.32587078626723,\n",
              " 80.30355898244291,\n",
              " 80.28190094247785,\n",
              " 80.26083933738877,\n",
              " 80.24032208651941,\n",
              " 80.22030187645501,\n",
              " 80.20073572403756,\n",
              " 80.18158457943835,\n",
              " 80.16281296561641,\n",
              " 80.14438865082703,\n",
              " 80.12628235115098,\n",
              " 80.10846746029215,\n",
              " 80.09091980414455,\n",
              " 80.07361741785733,\n",
              " 80.056540343336,\n",
              " 80.03967044530646,\n",
              " 80.02299124423975,\n",
              " 80.00648776459269,\n",
              " 79.99014639695974,\n",
              " 79.9739547728612,\n",
              " 79.95790165100931,\n",
              " 79.94197681400016,\n",
              " 79.9261709744755,\n",
              " 79.91047568988658,\n",
              " 79.89488328507147,\n",
              " 79.87938678192948,\n",
              " 79.86397983554188,\n",
              " 79.84865667614878,\n",
              " 79.83341205644467,\n",
              " 79.81824120370477,\n",
              " 79.80313977630054,\n",
              " 79.78810382420087]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsWAC1sCO2kp"
      },
      "source": [
        "loss_history = lr.loss_history"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_RgS6UIUI7V",
        "outputId": "091641ab-271a-4e61-ab01-c826f9415b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(loss_history)\n",
        "plt.title('MSE loss dependence on GD iteration')\n",
        "plt.xlabel('GD iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMHmslsmUI7W",
        "outputId": "23cfdcf0-77b2-47a0-bc3d-0a02dc00ec0d"
      },
      "source": [
        "mse_test = mse_loss(X_test, y_test, lr.w)\n",
        "print(mse_test)\n",
        "assert len(loss_history) <= 1000\n",
        "assert mse_test <= 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93.86575118282471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3rqBwb4UI7X"
      },
      "source": [
        "### Задание 4 <a id=\"task4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEhVdvZpUI7Y"
      },
      "source": [
        "Как известно, масштабирование данных может помочь в сходимости градиентного спуска (вспомните, почему). Отмасштабируйте имеющиеся данные с помощью мин-макс шкалирования:\n",
        "\n",
        "$$\n",
        "x^j = \\frac{x^j - \\min(x^j)}{\\max(x^j) - \\min(x^j)},\n",
        "$$\n",
        "\n",
        "где $x^j$ - это вектор значений признака.\n",
        "\n",
        "Учтите, что единичный признак нормировать не надо (почему?). Так что либо придумайте способ, как отнормировать все столбцы, кроме единичного.\n",
        "\n",
        "Примените линейную регрессию с градиентным спуском к данным, подберите оптимальную длину шага. Добейтесь того, чтобы значение MSE не превышало 100.\n",
        "\n",
        "Опишите наблюдения. Изменилась ли ситуация?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP9a7wslUI7Y"
      },
      "source": [
        "# scale the data\n",
        "# your code here\n",
        "\n",
        "\n",
        "# train linear regression with gradient descent\n",
        "lr_scaled = CustomLinearRegressionGD()\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6s8h52gUI7Z"
      },
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(loss_history)\n",
        "plt.title('MSE loss dependence on SGD iteration')\n",
        "plt.xlabel('SGD iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo3oY_0jUI7a"
      },
      "source": [
        "``` Опишите свои наблюдения.```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdwmhibtUI7a",
        "outputId": "5d3a9bf8-523e-4f59-bc17-106c5039f3f1"
      },
      "source": [
        "mse_test = mse_loss(X_test_scaled, y_test, lr_scaled.w)\n",
        "print(mse_test)\n",
        "assert mse_test <= 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90.76566264251974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASEVyc2oUI7b"
      },
      "source": [
        "# Часть 2 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY22qq7jUI7c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC1DQcP5UI7d"
      },
      "source": [
        "В этой части вы поработаете с данными из другого соревнования на Kaggle: https://www.kaggle.com/c/house-prices-advanced-regression-techniques. Задача - предсказание цены дома.\n",
        "\n",
        "Данные можно скачать [тут](https://raw.githubusercontent.com/WeaselCMC/ml_dpo_2021/master/hw/hw2/train.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45lqIuKrUI7e",
        "outputId": "941066ea-2e44-469d-b65f-218b36f70420"
      },
      "source": [
        "data = pd.read_csv('train.csv', header=0)\n",
        "data.drop('Id', axis=1, inplace=True)\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1460, 80)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brKRqJm5UI7e",
        "outputId": "475e2f70-bbb0-401a-8ae8-6171becbc9b3"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>...</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>NaN</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 80 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
              "0          60       RL         65.0     8450   Pave   NaN      Reg   \n",
              "1          20       RL         80.0     9600   Pave   NaN      Reg   \n",
              "2          60       RL         68.0    11250   Pave   NaN      IR1   \n",
              "3          70       RL         60.0     9550   Pave   NaN      IR1   \n",
              "4          60       RL         84.0    14260   Pave   NaN      IR1   \n",
              "\n",
              "  LandContour Utilities LotConfig  ... PoolArea PoolQC Fence MiscFeature  \\\n",
              "0         Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n",
              "1         Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n",
              "2         Lvl    AllPub    Inside  ...        0    NaN   NaN         NaN   \n",
              "3         Lvl    AllPub    Corner  ...        0    NaN   NaN         NaN   \n",
              "4         Lvl    AllPub       FR2  ...        0    NaN   NaN         NaN   \n",
              "\n",
              "  MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
              "0       0      2    2008        WD         Normal     208500  \n",
              "1       0      5    2007        WD         Normal     181500  \n",
              "2       0      9    2008        WD         Normal     223500  \n",
              "3       0      2    2006        WD        Abnorml     140000  \n",
              "4       0     12    2008        WD         Normal     250000  \n",
              "\n",
              "[5 rows x 80 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqH86-aoUI7f"
      },
      "source": [
        "### Задание 1 <a id=\"task21\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDhWjVyMUI7f"
      },
      "source": [
        "Есть ли в данных пропуски? Если да, то для каждого столбца, в котором они имеются, посчитайте их количество и их долю от общего числа значений. Что вы наблюдаете?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMSzy6x0UI7g"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUSIdxIsUI7g"
      },
      "source": [
        "``` Опишите свои наблюдения.```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VZxR2N7UI7g"
      },
      "source": [
        "### Задание 2 <a id=\"task22\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC_0pu24UI7g"
      },
      "source": [
        "Избавьтесь от пропусков. Заполните пропуски в колонках со значениями типа `object` (это можно проверить методом `.dtype` или `.dtypes`) отдельной категорией 'NaN', а в остальных (типа `float64` и `int64`) - средним по колонке.\n",
        "\n",
        "Проверьте, что вы действительно избавились от пропусков."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fM6ABwBUI7h"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYCeQ_GhUI7h"
      },
      "source": [
        "### Задание 3 <a id=\"task23\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC7oIeGAUI7h"
      },
      "source": [
        "Для каждой колонки, выраженной числами (типа `float64` и `int64`), постройте гистограмму ее значений. Сильно ли отличается масштаб признаков? Отмасштабируйте признаки каким-нибудь методом (например, `StandardScaler` или `MinMaxScaler` из `sklearn`). \n",
        "\n",
        "**Не забудьте, что целевая переменная (столбец `'SalePrice'`) не входит в признаки!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgNGoyMyUI7h"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aM57RfLUI7h"
      },
      "source": [
        "### Задание 4 <a id=\"task24\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFMWeWLjUI7i"
      },
      "source": [
        "Обработайте категориальные признаки. Примените к ним one-hot кодирование. Сколько получилось колонок у итогового датасета?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUJmsT79UI7i"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVSPa2inUI7i"
      },
      "source": [
        "### Задание 5 <a id=\"task25\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wqBy6qaUI7i"
      },
      "source": [
        "- Постройте гистограмму распределения предсказываемого значения. \n",
        "- Для избавления от разницы в масштабах, а также «смещения» распределения переменной в сторону нормального (что бывает полезно при статистическом анализе), можно прологарифмировать ее (это обратимое преобразование, поэтому целевую переменную легко восстановить). В данном случае воспользуйтесь `np.log1p`, чтобы сделать преобразование $y \\to \\ln\\left(1 + y\\right)$. \n",
        "- Постройте гистограмму распределения от нового предсказываемого значения. Опишите наблюдения.\n",
        "\n",
        "*В дальнейшем используйте в качестве предсказываемого значения вектор, который получился после логарифмирования.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjXqC1c1UI7j"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7ZTWPpOUI7j"
      },
      "source": [
        "``` Опишите свои наблюдения.```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDb2goh-UI7j"
      },
      "source": [
        "---\n",
        "Перейдем непосредственно к построению моделей. Разобьем выборку на обучение и контроль.\n",
        "\n",
        "*Пожалуйста, **не меняйте** значение `random_state` в следующей ячейке.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlTuB_MIUI7k"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(data.drop('SalePrice', axis=1), np.log1p(data['SalePrice']), random_state=13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWc5LPLSUI7k",
        "outputId": "09096ccb-f2bf-46bc-f372-aabd42be45c9"
      },
      "source": [
        "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1095, 79), (365, 79), (1095,), (365,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkq9wwKgUI7k"
      },
      "source": [
        "### Задание 6 <a id=\"task26\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfswcFJjUI7k"
      },
      "source": [
        "Перейдем к построению моделей машинного обучения с помощью `sklearn`. Обучите модель линейной регрессии на обучающей выборке и оцените качество по метрике **Root** Mean Squared Error (корень из MSE) на обучающей и валидационной выборках. Что вы наблюдаете? Как вы можете объяснить такой результат?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zXZX3XiUI7l"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGyEX4t5UI7l"
      },
      "source": [
        "``` Опишите свои наблюдения.```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gMDETG0UI7l"
      },
      "source": [
        "### Задание 7 <a id=\"task27\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEkW9KktUI7l"
      },
      "source": [
        "Теперь примените методы линейной регрессии с регуляризацией - Lasso (L1) и Ridge (L2). \n",
        "\n",
        "Для подбора коэффициента регуляризации будем использовать росс-валидацию (вспомните, что такое кросс-валидация и зачем она нужна). \n",
        "\n",
        "`GridSearchCV` - это метод, который перебирает значения гиперпараметров (по заданной «сетке»), считает для каждого качество на кросс-валидации и позволяет выбрать лучший гиперпараметр. Более подробно вы можете познакомиться с методом в [документаци](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
        "\n",
        "1. Прочитайте документацию `sklearn.model_selection.GridSearchCV`\n",
        "2. Подбирите оптимальный гиперпараметр для LASSO:\n",
        "    - В качестве сетки используйте `np.logspace(-5, 1)`\n",
        "    - Используйте 5-Fold Cross-Validation\n",
        "3. Используя оптимельное значение гиперпаметра обучите модель на всей обучающей выборке, посчитайте ошибку на тренировочной и обучающей выборках\n",
        "4. Повторите п. 2-3 для модели Ridge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNlU0ciGUI7m",
        "outputId": "d310f4cf-b9bb-48b2-f094-ee6e6b4fbf9d"
      },
      "source": [
        "# пример \"сетки\"\n",
        "np.logspace(-5, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.00000000e-05, 1.32571137e-05, 1.75751062e-05, 2.32995181e-05,\n",
              "       3.08884360e-05, 4.09491506e-05, 5.42867544e-05, 7.19685673e-05,\n",
              "       9.54095476e-05, 1.26485522e-04, 1.67683294e-04, 2.22299648e-04,\n",
              "       2.94705170e-04, 3.90693994e-04, 5.17947468e-04, 6.86648845e-04,\n",
              "       9.10298178e-04, 1.20679264e-03, 1.59985872e-03, 2.12095089e-03,\n",
              "       2.81176870e-03, 3.72759372e-03, 4.94171336e-03, 6.55128557e-03,\n",
              "       8.68511374e-03, 1.15139540e-02, 1.52641797e-02, 2.02358965e-02,\n",
              "       2.68269580e-02, 3.55648031e-02, 4.71486636e-02, 6.25055193e-02,\n",
              "       8.28642773e-02, 1.09854114e-01, 1.45634848e-01, 1.93069773e-01,\n",
              "       2.55954792e-01, 3.39322177e-01, 4.49843267e-01, 5.96362332e-01,\n",
              "       7.90604321e-01, 1.04811313e+00, 1.38949549e+00, 1.84206997e+00,\n",
              "       2.44205309e+00, 3.23745754e+00, 4.29193426e+00, 5.68986603e+00,\n",
              "       7.54312006e+00, 1.00000000e+01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG-jskQ9UI7m"
      },
      "source": [
        "# your Lasso code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKP7iZ0QUI7m"
      },
      "source": [
        "# your Ridge code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXPOrkSWUI7n"
      },
      "source": [
        "### Задание 8 <a id=\"task28\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0uT2Y90UI7n"
      },
      "source": [
        "Постройте гистограммы значений весов для линейной регрессии, Lasso и Ridge. Опишите наблюдения. В чем различия между полученными наборами весов и почему?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkW3TwdQUI7n"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRa0oEHAUI7n"
      },
      "source": [
        "``` Опишите свои наблюдения.```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y5J2dJDUI7o"
      },
      "source": [
        "### Задание 9 (Бонус) <a id=\"task29\"></a>\n",
        "\n",
        "\n",
        "- Скачайте тестовые данные из соревнования [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data?select=test.csv)\n",
        "- Примените к ним те же шаги предобработки (заполнение пропусков, масштабирование, и т.д.)\n",
        "- Выберите лучшую модель, которую вам удалось обучить и предскажите с ее помощью цену \n",
        "- На забудьте, что ваша модель предсказывает логарифм (см. Задание 5), так что нужно применить обраное преобразование\n",
        "- Загрузите ваши прогнозы на Kaggle (`Submit Prediction`) и узнайте их точность. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RI8I6-3UI7o"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}